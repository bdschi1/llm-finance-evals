"""
Risk logic rubric scoring

This module implements a simple rubric-based scorer for the
`risk_logic_v1` eval defined in `evals/risk_logic_eval.yaml`.

It mirrors the rubric dimensions in the YAML and returns:
- per-dimension scores (1–5),
- a weighted overall score (0–5),
- and optional reviewer notes.

At this stage, the implementation is deliberately simple; it serves as a
clear template for adding more sophisticated logic later.
"""

from dataclasses import dataclass
from typing import Dict, Any, Optional


@dataclass
class RubricDimension:
    id: str
    label: str
    weight: float  # fraction of 1.0


# These weights match the YAML config in evals/risk_logic_eval.yaml
RISK_LOGIC_DIMENSIONS = [
    RubricDimension(
        id="coverage_of_risks",
        label="Coverage of key risks",
        weight=0.35,
    ),
    RubricDimension(
        id="factor_and_exposure_logic",
        label="Factor and exposure logic",
        weight=0.30,
    ),
    RubricDimension(
        id="severity_and_impact",
        label="Severity and impact reasoning",
        weight=0.20,
    ),
    RubricDimension(
        id="structure_and_clarity",
        label="Structure and clarity",
        weight=0.15,
    ),
]


def _normalize_score(score: float) -> float:
    """
    Force scores into the 1–5 range.
    """
    return max(1.0, min(5.0, float(score)))


def score_risk_summary(
    model_output: str,
    golden_answer: Optional[str] = None,
    notes: Optional[str] = None,
) -> Dict[str, Any]:
    """
    Score a single risk summary according to the risk logic rubric.

    Currently implemented as a stub returning neutral scores (3.0)
    for all dimensions. A human reviewer or a separate judging step
    can later assign real scores per dimension.

    Parameters
    ----------
    model_output : str
        The risk summary generated by the model.
    golden_answer : str, optional
        Reference "golden" risk note. Not used yet.
    notes : str, optional
        Free-text reviewer notes.

    Returns
    -------
    Dict[str, Any]
        {
          "dimensions": {
             "<dimension_id>": {"score": float, "weight": float}
          },
          "overall_score": float,
          "model_output": str,
          "golden_answer": str or None,
          "reviewer_notes": str or None,
        }
    """
    base_score = 3.0  # neutral placeholder

    dimension_scores: Dict[str, Dict[str, float]] = {}
    overall = 0.0

    for dim in RISK_LOGIC_DIMENSIONS:
        s = _normalize_score(base_score)
        dimension_scores[dim.id] = {
            "score": s,
            "weight": dim.weight,
        }
        overall += s * dim.weight

    result: Dict[str, Any] = {
        "dimensions": dimension_scores,
        "overall_score": overall,
        "model_output": model_output,
        "golden_answer": golden_answer,
        "reviewer_notes": notes,
    }
    return result
